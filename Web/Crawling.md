# 웹 크롤링

### 🤖 크롤링
Web상에 존재하는 Contents를 수집하는 작업.  
페이지를 탐색하고 다른 페이지로 옮겨다니는 것(웹 크롤링), 도달한 페이지의 내용을 가져와 원하는 형태로 제공하는 것(웹 스크레이핑)을 묶어 크롤링이라고 한다.

1. HTML 페이지를 가져와서 HTML/CSS 등을 파싱하고 필요한 데이터만 추출하는 기법
2. Open API(Rest API)를 제공하는 서비스에 Open API를 호출해서 받은 데이터 중 필요한 데이터만 추출하는 기법
3. Selenium 등 브라우저를 프로그래밍으로 조작해서 필요한 데이터만 추출하는 기법

<br>

### 🤖 원하는 페이지 찾아 다니기
데이터를 구하기 위해서는 데이터가 존재하는 웹 페이지들을 찾아 다녀야 한다. 웹 페이지를 찾아다니는 방식을 크게 두 가지로 나눌 수 있다.

1. URL로 직접 접근
   - URL 주소의 패턴을 통해 다음 주소 파악
   - 해당 페이지에 존재하는 링크를 파싱해서 다음 주소를 파악

2. 웹 브라우저 사용
    - Selenium을 통해 웹 브라우저를 직접 컨트롤
    - PhantomJS 등 Headless 브라우저 사용

URL로 직접 접근하는 방식은 가장 단순하게 크롤링을 수행할 수 있다. 원하는 URL로 웹서버에 직접 요청을 보내고, 웹서버가 응답을 해주면 그 결과를 파싱해서 데이터를 추출한다. URL의 구조가 단순해서 패턴이 바로 보이는 경우, (예를 들면 target-web-page.com/page/10/ 등 순서를 유추할 수 있는 경우) 특정 문자열을 바꾸기만 해도 원하는 페이지로 이동할 수 있게 된다. 가능하다면 이렇게 URL로 직접 요청하는 방식을 사용하는 것이 좋다.

하지만 이와 같은 방식으로 원하는 결과물을 얻기가 쉽지 않을 때, 웹 브라우저의 도움을 얻는 것이다. 문제가 될 수 있는 상황은 대표적으로 다음과 같다.
- 로그인해서 데이터를 가져와야 한다
- 클릭, 폼 전송 등 사용자의 인터랙션이 필요하다
- 서버에서 보내는 HTML 결과물이 내가 보는 화면과 다르다

사용자의 액션이 필요하거나, 자바스크립트를 이용해 웹페이지를 동적으로 구성할 경우 단순하게 서버에 요청하는 것만으로는 원하는 결과물을 받기 어렵다.   
따라서 웹 브라우저를 직접 컨트롤해서 브라우저를 통해 결과물을 전달받는 방식을 사용한다. 대표적으로 **Selenium**을 많이 사용한다.  
Selenium을 사용하면 크롬/파이어폭스/사파리 등 원하는 브라우저를 파이썬/R 코드를 통해 동작시킬 수 있다. 클릭, 키보드 입력 등을 모두 코드로 구현할 수 있기 때문에 사용자의 액션을 따라할 수 있다. 또한 브라우저가 JS 코드를 동작시켜서 HTML을 동적으로 렌더링 해주기 때문에 동적으로 구성되는 페이지라도 완전한 결과물을 확인할 수 있다.  
액션이 진행되는 과정을 눈으로 확인하려면 Selenium이 동작시키는 브라우저 화면을 보면 된다. 하지만 그런 과정 확인 없이 동작만 잘하면 된다 라고 생각하면, PhantomJS 등 **Headless Browser**를 사용하면 된다.  
Headless Browser는 화면이 존재하지 않는 웹 브라우저라고 생각하면 된다. 화면이 존재하지 않기 때문에 눈에 보이지는 않지만 웹 브라우저라서 동적으로 구성되는 페이지도 렌더링할 수 있다. 따라서 서버 상에서 동작시키거나 굳이 화면을 켜놓고 싶지 않은 상황에서 사용한다.

<br>

### 🤖 웹페이지의 데이터 가져오기 (요청)
웹 브라우저를 통해 직접 접근하게 되면 요청을 우리가 직접 할 필요는 없다. 하지만 URL로 직접 연결해야 할 경우, 서버에 적절한 요청을 하지 못하면 제대로 된 응답을 받지 못할 수 있다.  
보통 HTTP 메서드 중에서도 GET을 많이 사용하게 되고, 가끔 필요한 경우 POST 요청을 하게 될 수 있다. GET 요청의 경우 URL 뒤에 모든 파라미터가 붙고, POST 요청의 경우 전송되는 파라미터를 URL 상으로는 알 수 없다. (크롬 개발자 도구 중 네트워크 항목에서 확인 가능)

[*GET과 POST의 차이*](https://blog.outsider.ne.kr/312)

<br>

### 🤖 웹페이지의 데이터 가져오기 (결과)
어떤 방식을 사용해서든지 원하는 HTML 결과를 얻었다고 생각해보자. (결과물이 XML 또는 json일 수 있지만, 일단 HTML에 대해서만 생각해보자) 하지만 우리가 얻어낸 것은 태그들이 가득한 문자열이다. 이 문자열로부터 원하는 데이터를 얻기 위해서는 HTML을 파싱할 수 있는 도구를 사용해야 한다. 일반적으로 파이썬을 사용해 HTML을 처리한다면 BeautifulSoup을 사용하게 된다. (R은 rvest나 RCurl 등을 사용한다)
 
<br>

### 🤖 웹크롤링
특정 사이트에서 내가 원하는 정보를 가져오는 행위이다.

```python
find("태그명", {"class":"찾고자 하는 클래스명"})
```

이렇게 작성하면 우리가 원하는 값을 가지고 온다.
ex) **find("strong", {"class":"num"})** : strong이라는 태그에서 num 클래스를 찾겠다(find)

<br>

### 🤖 BeautifulSoup 라이브러리를 활용한 크롤링
- HTML 태그를 파싱해서 필요한 데이터만 추출하는 함수 제공 라이브러리
- 설치 방법 : pip install bs4

```python
import requests
from bs4 import BeautifulSoup

# 1) reqeusts 라이브러리를 활용한 HTML 페이지 요청 
# 1-1) res 객체에 HTML 데이터가 저장되고, res.content로 데이터를 추출할 수 있음
res = requests.get('http://v.media.daum.net/v/20170615203441266')

# print(res.content)
# 2) HTML 페이지 파싱 BeautifulSoup(HTML데이터, 파싱방법)
# 2-1) BeautifulSoup 파싱방법
soup = BeautifulSoup(res.content, 'html.parser')

# 3) 필요한 데이터 검색
title = soup.find('title')

# 4) 필요한 데이터 추출
print(title.get_text())
```


<br>

### 참고
[웹크롤링 기본: 크롤링 이해 및 기본](https://www.fun-coding.org/crawl_basic2.html#gsc.tab=0)  
[웹크롤링 기초 개념과 코드 구현-티스토리](https://auto-trading.tistory.com/entry/%ED%8A%B9%EA%B0%95-%EC%9B%B9%ED%81%AC%EB%A1%A4%EB%A7%81Web-Crowling-%EA%B8%B0%EC%B4%88-%EA%B0%9C%EB%85%90%EA%B3%BC-%EC%BD%94%EB%93%9C-%EA%B5%AC%ED%98%84with-Python)  
[웹 크롤링에 대해서](https://lumiamitie.github.io/web/introduction-to-web-crawling/)
